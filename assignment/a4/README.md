# Assignment 4: Multimodal Processing

This assignment consists of the following:
* [Image_Captioning](image_captioning.ipynb), where you'll examine a classic image captioning dataset. You'll also read one of the classic image captioning papers and answer some questions about their architecture. Finally, you'll experiment with CLIP embeddings to categorize the content of images.
* [answers](answers) file where you'll put all your answers (except D5)



## Submission Instructions

This assignment does **not** require you to use a Colab notebook. You can run it all on your GCP instance and then use the submit.sh script to submit your work. As a result you do **not** need to copy notebooks this time.

As with Assignment 3, please submit by running the submit.sh script, only with `-a 4` (since this is assignment 4).
```
./assignment/submit.sh -u your-github-username -a 4
```

It is your responsibility to check that your work has made it to your GitHub repository in the `a4-submit` branch.  Remember that we will grade by looking at the content of your answers file as well as your notebook.

As always, a small number of points are awarded in each assignment for submitting in the right place. We will give each person who correctly submits their assignment one point on this homework assignment. We will also give one point to each person who submits an answer file that is parseable by the autograder (e.g. properly filled out as you did in a0 and a1).
